{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Computation Exercise 2: Linear Regression and Gradient Descent\n",
    "\n",
    "In this exercise, we'll go through another example of linear regression from an implementation\n",
    "perspective. We will use the `Boston Housing dataset`, and predict the median cost of a home\n",
    "in an area of Boston. In this exercise, you will learn the following\n",
    "* set up the linear regression problem using numpy\n",
    "* show that vectorized code is faster\n",
    "* produce scatter and line plots using Matplotlib\n",
    "* solve the linear regression problem using the closed form solution\n",
    "* solve the linear regression problem using gradient descent\n",
    "\n",
    "We will use the two Python packages [NumPy](http://www.numpy.org/) and [Matplotlib](https://matplotlib.org/). NumPy is an open-source module that provides fast, precompiled numerical routines. To learn more about NumPy, you can [read this short tutorial](https://numpy.org/doc/stable/user/quickstart.html). Matplotlib is a 2D plotting library which can be used to produce [a wide range of plots](https://matplotlib.org/2.0.2/gallery.html), including histograms, power spectra, bar charts, errorcharts, and scatterplots. To learn more about Maptplotlib, you can [read this short tutorial](https://matplotlib.org/2.0.2/users/pyplot_tutorial.html). \n",
    "\n",
    "You should import these packages using the `import` statement. To call a function `X` from the NumPy module, you would normally have to write `NumPy.X()`. However, if you invoke NumPy functions many places in your code, this quickly becomes tedious. By adding `as np` after your `import` statement as shown below, you can instead write  `np.X()`, which is less verbose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import preprocessing   # for normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Housing Data\n",
    "\n",
    "The Boston Housing data is one of the \"toy datasets\" available in sklearn.\n",
    "We can import and display the dataset description like this:\n",
    "\n",
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.datasets import load_boston\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    boston_data = load_boston()\n",
    "print(boston_data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the example simple, we will only work with **two** features: `INDUS`\n",
    "and `RM`. The explanations of these features are in the description above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the boston data\n",
    "data = boston_data['data']\n",
    "print(data.shape)\n",
    "# we will only work with two of the features: INDUS and RM\n",
    "x_input = data[:, [2,5]]\n",
    "y_target = boston_data['target']\n",
    "# we normalize the data so that each has regularity\n",
    "x_input = preprocessing.normalize(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step, we scale input vectors individually to unit norm (vector length). The goal of normalization is to change the values in the dataset to a common scale, which is key to get more robust results. There are several different noramlization strategies. More details can be found [here](https://scikit-learn.org/stable/modules/preprocessing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Visualization\n",
    "\n",
    "Just to give us an intuition of how these two features INDUS and RM\n",
    "affect housing prices, lets visualize the feature interactions.\n",
    "As expected, the more \"industrial\" a neighbourhood is, the lower the\n",
    "housing prices. The more rooms houses in a neighbourhood have, the\n",
    "higher the median housing price.\n",
    "\n",
    "We will now visualise the dataset using a scatter plot using the [`scatter`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html) function in the matplotlib.pylot module. This function can be called as follows:\n",
    "\n",
    "    plt.scatter( x , y )\n",
    "    \n",
    "The two arguments `x` and `y` are the input data. We can label the `x`- and `y`-axes as follows: \n",
    "\n",
    "    plt.xlabel(\"x_label_here\")\n",
    "    plt.ylabel(\"y_label_here\") \n",
    "    \n",
    "The function scatter has many additional arguments, as [described in the reference manual]([`scatter`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html). \n",
    "\n",
    "Now, you should make a scatter plot of the `price` versus `Industrialness`. You should label the x- and y-axes by \"Industrialness\" and \"Med House Price\". Hint: Remember to include `plt.show()` at the end; otherwise, the scatter plot is not shown.\n",
    "In a similar way, you can make a scatter plot of the `price` versus `Avg Num Rooms`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual plots for the two features:\n",
    "plt.title('Industrialness vs Med House Price')\n",
    "plt.scatter(x_input[:, 0], y_target)\n",
    "plt.xlabel('Industrialness')\n",
    "plt.ylabel('Med House Price')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Avg Num Rooms vs Med House Price')\n",
    "plt.scatter(x_input[:, 1], y_target)\n",
    "plt.xlabel('Avg Num Rooms')\n",
    "plt.ylabel('Med House Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Linear Regression Model\n",
    "\n",
    "A linear regression model in our problem has the following form \n",
    "$$\n",
    "f(x)=\\mathbf{w}^\\top \\mathbf{x}+b=w_{1}x_{1}+w_{2}x_{2}+b,\n",
    "$$\n",
    "where $\\mathbf{x}$ is the input, $\\mathbf{w}$ is called the weight and $b$ is known as the bias.\n",
    "The purpose of generating such a model is to predict an output (price) given an input (Industrialness, Avg Num Rooms). Given the model parameter $\\mathbf{w},b$ and the new input $x$, the output predicted by our simple model is $\\mathbf{w}^\\top \\mathbf{x}+b$. We will define a function named `linearmodel(x,w)` which represents this model. The function takes three arguments, the weight parameter $\\mathbf{w}$, the bias parameter $b$ and the input $\\mathbf{x}$, and it returns the predicted output $\\mathbf{w}^\\top \\mathbf{x}+b$. \n",
    "\n",
    "A function is a block of organized, reusable code that is used to perform a single, related action. Like Java or C, you can declare your own function in Python. Function blocks usually begin with the keyword def followed by the function name and parentheses. \n",
    "Any input parameters or arguments should be placed within these parentheses. You can also define parameters inside these parentheses. A return statement with no arguments (i.e. return;) is the same as return None. \n",
    "\n",
    "    def function_name( parameters ):\n",
    "       return value\n",
    "\n",
    "Notice that Python programs get structured through indentation, i.e. code blocks are defined by their indentation. This principle makes it easier to read and understand other people's Python code, but sometimes it could cause confusion to some people, especially those who are used to using { } to specify a code blocks, like in Java or C. Note also that Python does not require a semi-colon ; at the end of each statement. \n",
    "\n",
    "Now, you should define the function `linearmodel` as described above. Note we use the dot function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices. \n",
    "    \n",
    "    np.dot(w, v) for vector dot produt\n",
    "    np.dot(W, V) for matrix dot product\n",
    "    \n",
    "We require you to complete the following code to compute the predicted output of linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: Insert code to define the linearmodel function here.\n",
    "def linearmodel(w, b, x):\n",
    "    '''\n",
    "    Input: w is a weight parameter, b is a bias parameter, and x is d-dimensional vector (representing a example)\n",
    "    Output: the predicted output\n",
    "    '''\n",
    "    return np.dot(w, x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `linearmodel` gives a prediction on a single example $\\mathbf{x}$. It is often the case that we need to provide predictions on several examples $$(\\mathbf{x}^{(1)},y^{(1)}),\\ldots,(\\mathbf{x}^{(n)},y^{(n)})$$ simultaneously. We therefore collect $n$ training examples $(\\mathbf{x}^{(1)},y^{(1)}),\\ldots,(\\mathbf{x}^{(n)},y^{(n)})$ into an input matrix $X\\in\\mathbb{R}^{n\\times d}$ ($d$ is the number of features) and a vector $\\mathbf{y}\\in\\mathbb{R}^n$. That is\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "    (\\mathbf{x}^{(1)})^\\top \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}^{(n)})^\\top\n",
    "  \\end{pmatrix}=\\begin{pmatrix}\n",
    "    x_1^{(1)} & x_2^{(1)} & \\ldots & x_d^{(1)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    x_1^{(n)} & x_2^{(n)} & \\ldots & x_d^{(n)}\n",
    "  \\end{pmatrix},\\quad \\mathbf{y}=\\begin{pmatrix}\n",
    "    y^{(1)} \\\\\n",
    "    \\vdots \\\\\n",
    "    y^{(n)}.\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "Given the data matrix $X\\in\\mathbb{R}^{n\\times d}$, write a function to compute the output $\\mathbf{t}=(t^{(1)},\\ldots,t^{(n)})^\\top$, where $t^{(i)}$ is the output of the linear model $(\\mathbf{w},b)$ on $\\mathbf{x}$, \n",
    "i.e., $t^{(i)}=\\mathbf{w}^\\top \\mathbf{x}^{(i)}+b$. A direct idea is to use the `for` loop to traverse all training examples. We request you to finish the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearmat_1(w, b, X):\n",
    "    '''\n",
    "    Input: w is a weight parameter, b is a bias parameter, and X is a data matrix (n x d)\n",
    "    Output: a vector containing the predictions of linear models\n",
    "    '''\n",
    "    # n is the number of training examples\n",
    "    n = X.shape[0]\n",
    "    t = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        # To do: Insert your code to compute the predicted output for the i-th example, and assign it to t[i]\n",
    "        t[i] = linearmodel(w, b, X[i, :])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In the function `linearmat_1`, we do prediction by traversing all training examples one by one. This implementation is very slow. Python provides much more efficient implementation in terms of vectorization. By vectorization we mean that we write the prediction in terms of matrix. In Python, vectorized code written in numpy tend to be faster than code that uses a `for` loop. We now show how to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture, we can absorb the bias into the weight vector by adding a feature of `1`. The benefit is that we do not need to consider separately the bias parameter and the weight parameter. That is,\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "    1 & (\\mathbf{x}^{(1)})^\\top \\\\\n",
    "    \\vdots \\\\\n",
    "    1 & \\mathbf{x}^{(n)})^\\top\n",
    "  \\end{pmatrix}=\\begin{pmatrix}\n",
    "    1 & x_1^{(1)} & x_2^{(1)} & \\ldots & x_d^{(1)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    1 & x_1^{(n)} & x_2^{(n)} & \\ldots & x_d^{(n)}\n",
    "  \\end{pmatrix},\\quad \\mathbf{y}=\\begin{pmatrix}\n",
    "    y^{(1)} \\\\\n",
    "    \\vdots \\\\\n",
    "    y^{(n)}\n",
    "  \\end{pmatrix}.\n",
    "$$\n",
    "In this case, the predictions $\\mathbf{t}$ can be written in terms of a matrix multiplication\n",
    "$$\n",
    "\\mathbf{t}=X\\mathbf{w}=\\begin{pmatrix}\n",
    "    1 & (\\mathbf{x}^{(1)})^\\top \\\\\n",
    "    \\vdots \\\\\n",
    "    1 & \\mathbf{x}^{(n)})^\\top\n",
    "  \\end{pmatrix}\\mathbf{w}=\\begin{pmatrix}\n",
    "    1 & x_1^{(1)} & x_2^{(1)} & \\ldots & x_d^{(1)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    1 & x_1^{(n)} & x_2^{(n)} & \\ldots & x_d^{(n)}\n",
    "  \\end{pmatrix}\\begin{pmatrix}\n",
    "  b\\\\\n",
    "  w_1\\\\\n",
    "  \\vdots\\\\\n",
    "  w_d\n",
    "  \\end{pmatrix}, \\quad\\text{where we use a new notation }\\;\\mathbf{w}=\\begin{pmatrix}\n",
    "  b\\\\\n",
    "  w_1\\\\\n",
    "  \\vdots\\\\\n",
    "  w_d\n",
    "  \\end{pmatrix}.\n",
    "$$\n",
    "Note here we include the bias in the weight vector $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearmat_2(w, X):\n",
    "    '''\n",
    "    a vectorization of linearmat_1.\n",
    "    Input: w is a weight parameter (including the bias), and X is a data matrix (n x (d+1)) (including the feature)\n",
    "    Output: a vector containing the predictions of linear models\n",
    "    '''\n",
    "    \n",
    "    # To do: Insert you code to get a vectorization of the predicted output computation for a linear model\n",
    "    return np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing speed of the vectorized vs unvectorized code\n",
    "\n",
    "We'll see below that the vectorized code already\n",
    "runs much faster than the non-vectorized code! \n",
    "\n",
    "Hopefully this will convince you to always vectorized your code whenever possible. We first import `time` module to include various time-related functions. The time() function returns the current system time in ticks since 00:00:00 hrs January 1, 1970(epoch).\n",
    "\n",
    "Time for non-vectorized code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "w = np.array([1,1])\n",
    "b = 1\n",
    "t0 = time.time()\n",
    "p1 = linearmat_1(w, b, x_input)\n",
    "t1 = time.time()\n",
    "print('the time for non-vectorized code is %s' % (t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for vectorized code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add the bias to the weight vector (wb means weights with bias)\n",
    "wb = np.array([b, w[0], w[1]]) \n",
    "# add an extra feature (column in the input) that are just all ones\n",
    "x_in = np.concatenate([np.ones([np.shape(x_input)[0], 1]), x_input], axis=1)\n",
    "t0 = time.time()\n",
    "p2 = linearmat_2(wb, x_in)\n",
    "t1 = time.time()\n",
    "print('the time for vectorized code is %s' % (t1 - t0))\n",
    "print('diff in two implementations is %s' % np.dot(p2 - p1, p2 - p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **vectorization** is much faster than **non-vectorization**. Also, these two approaches yield almost the same results: the difference is less than $10^{-28}$.|\n",
    "\n",
    "## Defining the Cost Function\n",
    "\n",
    "In lecture, we defined the cost function for a linear regression problem using the square loss:\n",
    "\n",
    "$$C(\\mathbf{y}, \\mathbf{t}) = \\frac{1}{2n} \\sum_{i=1}^n (y^{(i)}-t^{(i)})^2,$$\n",
    "where $y^{(i)}$ is the $i$-th true output and $t^{(i)}$ is the $i$-th predicted output.\n",
    "\n",
    "As we discussed in the lecture, this can be written as\n",
    "$$\n",
    "C(\\mathbf{y}, \\mathbf{t}) = \\frac{1}{2n}(\\mathbf{y}-\\mathbf{t})^\\top (\\mathbf{y}-\\mathbf{t}).\n",
    "$$\n",
    "Use this equation to define the cost function for the linear regression problem. Note that $\\mathbf{v}^\\top\\mathbf{v}$ should be implemented by the function `np.dot`. The underlying reason is that NumPy's transpose() effectively reverses the shape of an array. If the array is one-dimensional, this means it has no effect. Therefore, if `v` is a one-dimensional array in python, `v.T` is the same as $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w, X, y):\n",
    "    '''\n",
    "    Evaluate the cost function in a vectorized manner for \n",
    "    inputs `X` and outputs `y`, at weights `w`.\n",
    "    '''\n",
    "    # TODO: Insert your code to compute the cost\n",
    "    residual = y - linearmat_2(w, X)  # get the residual\n",
    "    err = np.dot(residual, residual) / (2 * len(y))\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the cost for this hypothesis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(wb, x_in, y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting cost in weight space\n",
    "\n",
    "We'll plot the cost for two of our weights, assuming that bias = 31.11402451. We'll see where that number comes from later.\n",
    "Notice the shape of the contours are ovals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign some values to $w$, using a `np.arange(start, stop, step)` function call as follows:\n",
    "\n",
    "    w1s = np.arange(-22, -10, 0.01)\n",
    "    w2s = np.arange(0, 12, 0.1)\n",
    "\n",
    "Then we use `np.meshgrid(w1s, w2s)` to build a coordinate system, which is a matrix and each element gives a $(w_1, w_2)$ pair. For each $(w_1, w_2)$, we then apply the `cost` function to compute the cost at this weight vector and therefore get a cost matrix. This is achieved by a double `for` loop. More details about `meshgrid` can be found at [here](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html)\n",
    "\n",
    "Finally we apply the `plt.contour(W1, W2, z_cost, 25)` to plot the contour, where points in each curve achieves the same `cost`. We also label the x-, y-axis and give a title.\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1s = np.arange(-22, -10, 0.01)\n",
    "w2s = np.arange(0, 12, 0.1)\n",
    "b = 31.11402451    \n",
    "W1, W2 = np.meshgrid(w1s, w2s)\n",
    "z_cost = np.zeros([len(w2s), len(w1s)])  \n",
    "for i in range(W1.shape[0]):\n",
    "    for j in range(W1.shape[1]):\n",
    "        w = np.array([b, W1[i, j], W2[i, j]])\n",
    "        z_cost[i, j] = cost(w, x_in, y_target)\n",
    "CS = plt.contour(W1, W2, z_cost,25)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.title('Costs for various values of w1 and w2 for b=31.11402451')\n",
    "plt.xlabel(\"w1\")\n",
    "plt.ylabel(\"w2\")\n",
    "plt.plot([-16.44307658], [6.79809451], 'o') # this will be the minima that we'll find later\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Solution\n",
    "\n",
    "In the lecture, we show that the liner regression problem has a closed-form solution\n",
    "$$\n",
    "\\mathbf{w}^*=(X^\\top X)^{-1}X^\\top y.\n",
    "$$\n",
    "We now implement the **exact solution** in python. To this aim, we need to compute the `inverse` of a matrix. Python has provided a function to this aim. \n",
    "\n",
    "`np.linalg.inv(A)` computes the inverse of the matrix $A$.  We require you to complete the following code to compute the exact solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_exactly(X, y):\n",
    "    '''\n",
    "    Solve linear regression exactly. (fully vectorized)\n",
    "    \n",
    "    Given `X` - n x (d+1) matrix of inputs\n",
    "          `y` - target outputs\n",
    "    Returns the optimal weights as a (d+1)-dimensional vector\n",
    "    '''\n",
    "    # TODO: Insert your code to return the exact solution\n",
    "    A = np.dot(X.T, X)\n",
    "    c = np.dot(X.T, y)\n",
    "    return np.dot(np.linalg.inv(A), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_exact = solve_exactly(x_in, y_target)\n",
    "print(w_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is clear why we choose bias = 31.11402451 in the visualization of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Function and Gradient Descent\n",
    "\n",
    "In this final section, we are going to use the gradient descend method to find  $w^{*} = \\text{arg min}_w C(w)$, that is, the value of parameter $w$ such that $C(w)$ reaches the (locally) minimum value. In gradient descent, we usually work with an *iterative update scheme* for the weight $w$: \n",
    "$$\\mathbf{w}^{(t+1)} \\leftarrow \\mathbf{w}^{(t)} - \\eta\\nabla C(\\mathbf{w}^{(t)}),$$ \n",
    "where $\\eta$ is a learning rate and $\\nabla C(w^{(t)})$ is the gradient evaluated at current parameter value $\\mathbf{w}^{(t)}$. \n",
    "\n",
    "In order to implement gradient descent, we need to be able to compute the *gradient*\n",
    "of the cost function with respect to a weight $\\mathbf{w}$. In the lecture, we have derived the following closed-form solution of the gradient:\n",
    "\n",
    "$$\\nabla C(\\mathbf{w}) = \\frac{1}{n}\\big(X^\\top X\\mathbf{w}-X^\\top\\mathbf{y}\\big)=\\frac{1}{n}X^\\top\\big(X\\mathbf{w}-\\mathbf{y}\\big)$$ We require you to complete the following code for the gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized gradient function\n",
    "def gradfn(weights, X, y):\n",
    "    '''\n",
    "    Given `weights` - a current \"Guess\" of what our weights should be\n",
    "          `X` - matrix of shape (N,d+1) of input features including the feature $1$\n",
    "          `y` - target y values\n",
    "    Return gradient of each weight evaluated at the current value\n",
    "    '''\n",
    "    # TODO: Insert your code to return the gradient\n",
    "    y_pred = np.dot(X, weights)\n",
    "    residual = y_pred - y\n",
    "    return np.dot(X.T, residual) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, we can solve the optimization problem by repeatedly\n",
    "applying gradient descent on $w$. We requie you to complete the following code for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_via_gradient_descent(X, y, print_every=500,\n",
    "                               niter=10000, eta=1):\n",
    "    '''\n",
    "    Given `X` - matrix of shape (N,D) of input features\n",
    "          `y` - target y values\n",
    "          `print_every` - we report performance every 'print_every' iterations\n",
    "          `niter` - the number of iterates allowed\n",
    "          `eta` - learning rate\n",
    "    Solves for linear regression weights.\n",
    "    Return weights after `niter` iterations.\n",
    "    '''\n",
    "    N, D = np.shape(X)\n",
    "    # initialize all the weights to zeros\n",
    "    w = np.zeros([D])\n",
    "    idx_res = []\n",
    "    err_res = []\n",
    "    for k in range(niter):\n",
    "        # TODO: Insert your code to update w by gradient descent\n",
    "        dw = gradfn(w, X, y)\n",
    "        w = w - eta * dw\n",
    "        if k % print_every == print_every - 1:\n",
    "            print('Weight after %d iteration: %s' % (k, str(w)))\n",
    "            idx_res.append(k)\n",
    "            err_res.append(cost(w, X, y))\n",
    "    plt.plot(idx_res, err_res, color=\"red\", linewidth=2.5, linestyle=\"-\")\n",
    "    #plt.xscale('log')\n",
    "    #plt.yscale('log')\n",
    "    plt.show()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_via_gradient_descent( X=x_in, y=y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, gradient descent find solutions very similar to the exact solution. This shows that gradient descent is an effective method to find the best linear model in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
